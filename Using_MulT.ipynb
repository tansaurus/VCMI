{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tansaurus/VCMI/blob/main/Using_MulT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIhkQud_y0PB",
        "outputId": "e83e81f1-fefe-4122-9eaa-2847d21106bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.10/dist-packages (2.14.1)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.2.5)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.13.2)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.2.1)\n",
            "Requirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (7.1.0)\n",
            "Requirement already satisfied: entrypoints<1 in /usr/local/lib/python3.10/dist-packages (from mlflow) (0.4)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.1.43)\n",
            "Requirement already satisfied: graphene<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.3)\n",
            "Requirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (7.1.0)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.6)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.7.1)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.25.2)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.25.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.25.0)\n",
            "Requirement already satisfied: packaging<25 in /usr/local/lib/python3.10/dist-packages (from mlflow) (24.1)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.0.3)\n",
            "Requirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.20.3)\n",
            "Requirement already satisfied: pyarrow<16,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (14.0.2)\n",
            "Requirement already satisfied: pytz<2025 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2023.4)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.10/dist-packages (from mlflow) (6.0.1)\n",
            "Requirement already satisfied: querystring-parser<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.2.4)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.31.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.2.2)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.11.4)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.0.31)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (0.5.0)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.1.4)\n",
            "Requirement already satisfied: gunicorn<23 in /usr/local/lib/python3.10/dist-packages (from mlflow) (22.0.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.5)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic!=1.10.0,<2->mlflow) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker<8,>=4.0.0->mlflow) (2.0.7)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (3.0.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython<4,>=3.1.9->mlflow) (4.0.11)\n",
            "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.10/dist-packages (from graphene<4->mlflow) (3.2.3)\n",
            "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.10/dist-packages (from graphene<4->mlflow) (3.2.0)\n",
            "Requirement already satisfied: aniso8601<10,>=8 in /usr/local/lib/python3.10/dist-packages (from graphene<4->mlflow) (9.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow) (3.19.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2.11->mlflow) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (2.8.2)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow) (1.2.14)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.46b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow) (0.46b0)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3->mlflow) (2024.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from querystring-parser<2->mlflow) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow) (2024.6.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (3.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.0.3)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow) (1.14.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow) (5.0.1)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.18.0+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.23.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.15.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->timm) (12.5.40)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install mlflow\n",
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "12Sd4h1co3GA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8244d5b5-3d8f-47a3-91c8-a4e6e1a4d61c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6akDWsR6pLwC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d95be89-b2a3-48ce-9a2e-f27a62e2bef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Jun 28 12:40:21 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8c4zLtrupO_h"
      },
      "outputs": [],
      "source": [
        "# 미래 버전의 Python 기능을 현재 버전에서 사용하도록 설정\n",
        "from __future__ import print_function\n",
        "\n",
        "# 데이터 조작 및 계산을 위한 라이브러리\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 머신러닝 및 데이터 분석 라이브러리\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 시각화 라이브러리\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "# 딥러닝 및 이미지 처리 라이브러리\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "# 진행률 표시를 위한 라이브러리\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 파일 및 디렉토리 작업을 위한 라이브러리\n",
        "import os\n",
        "import glob\n",
        "import zipfile\n",
        "\n",
        "# 기타\n",
        "from itertools import chain\n",
        "import random\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import shutil\n",
        "\n",
        "#mlflow\n",
        "from mlflow import log_metric, log_param, log_artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DdJLzgfFruv9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import mlflow\n",
        "\n",
        "# MLflow 디렉토리 생성\n",
        "mlflow_dir = '/content/drive/MyDrive/Colab Notebooks/VCMI/20240702/mlruns'\n",
        "os.makedirs(mlflow_dir, exist_ok=True)\n",
        "\n",
        "# MLflow 설정\n",
        "os.environ['MLFLOW_TRACKING_URI'] = mlflow_dir\n",
        "mlflow.set_tracking_uri(mlflow_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BPo4cUdhsAxa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4ae85765-7256-40b6-9d26-5fc36818f800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://2u7k82ck402-496ff2e9c6d22116-5000-colab.googleusercontent.com/\n"
          ]
        }
      ],
      "source": [
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(5000)\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yiQYLJyMzJk4"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "experiment_name = \"MulT Optimization Acc\"\n",
        "mlflow.set_experiment(experiment_name)\n",
        "\n",
        "params = {\n",
        "    \"lr\": 3e-7,\n",
        "    \"batch_size\": 16,\n",
        "    \"gamma\": 0.8,\n",
        "    \"epochs\": 100,\n",
        "    \"seed\": 42\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oc8mJPOfpdWD"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(params[\"seed\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "h6OTrCSRpf9U"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gwtewLazqYF6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_data_ADC = pd.read_csv('/content/drive/My Drive/Colab Notebooks/VCMI/monomodal/ADC/train/train_labels.csv')\n",
        "valid_data_ADC = pd.read_csv('/content/drive/My Drive/Colab Notebooks/VCMI/monomodal/ADC/val/val_labels.csv')\n",
        "test_data_ADC = pd.read_csv('/content/drive/My Drive/Colab Notebooks/VCMI/monomodal/ADC/test/test_labels.csv')\n",
        "\n",
        "train_data_DWI = pd.read_csv('/content/drive/My Drive/Colab Notebooks/VCMI/monomodal/DWI/train/train_labels.csv')\n",
        "valid_data_DWI = pd.read_csv('/content/drive/My Drive/Colab Notebooks/VCMI/monomodal/DWI/val/val_labels.csv')\n",
        "test_data_DWI = pd.read_csv('/content/drive/My Drive/Colab Notebooks/VCMI/monomodal/DWI/test/test_labels.csv')\n",
        "\n",
        "train_data_T2 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/VCMI/monomodal/T2/train/train_labels.csv')\n",
        "valid_data_T2 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/VCMI/monomodal/T2/val/val_labels.csv')\n",
        "test_data_T2 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/VCMI/monomodal/T2/test/test_labels.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7DEKs8k0qwrb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import timm\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "class Dataset(Dataset):\n",
        "    def __init__(self, adc_df, dwi_df, t2_df, transform=None):\n",
        "        self.adc_df = adc_df\n",
        "        self.dwi_df = dwi_df\n",
        "        self.t2_df = t2_df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.adc_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        adc_path = self.adc_df.iloc[idx, 0]\n",
        "        dwi_path = self.dwi_df.iloc[idx, 0]\n",
        "        t2_path = self.t2_df.iloc[idx, 0]\n",
        "\n",
        "        adc_image = Image.open(adc_path).convert('RGB')\n",
        "        dwi_image = Image.open(dwi_path).convert('RGB')\n",
        "        t2_image = Image.open(t2_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            adc_image = self.transform(adc_image)\n",
        "            dwi_image = self.transform(dwi_image)\n",
        "            t2_image = self.transform(t2_image)\n",
        "\n",
        "        label = int(self.adc_df.iloc[idx, 1])\n",
        "\n",
        "        return adc_image, dwi_image, t2_image, label\n",
        "\n",
        "def create_dataloader(adc_df, dwi_df, t2_df, batch_size, transform=transform):\n",
        "    dataset = Dataset(adc_df, dwi_df, t2_df, transform=transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    return dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "w6zW5hAQsFXZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import timm\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "\n",
        "from typing import Optional, Any\n",
        "from argparse import Namespace\n",
        "import math\n",
        "from torch.nn import Parameter\n",
        "\n",
        "\n",
        "class MulT(nn.Module):\n",
        "    def __init__(self, hyp_params):\n",
        "        \"\"\"\n",
        "        Construct a MulT model.\n",
        "        \"\"\"\n",
        "        super(MulT, self).__init__()\n",
        "        self.orig_d_adc, self.orig_d_dwi, self.orig_d_t2 = hyp_params.orig_d_adc, hyp_params.orig_d_dwi, hyp_params.orig_d_t2\n",
        "        self.d_l, self.d_a, self.d_v = 32, 32, 32\n",
        "        self.adc_only = int(hyp_params.adc_only)\n",
        "        self.dwi_only = int(hyp_params.dwi_only)\n",
        "        self.t2_only = int(hyp_params.t2_only)\n",
        "        self.num_heads = hyp_params.num_heads\n",
        "        self.layers = hyp_params.layers\n",
        "        self.attn_dropout = hyp_params.attn_dropout\n",
        "        self.attn_dropout_dwi = hyp_params.attn_dropout_dwi\n",
        "        self.attn_dropout_t2 = hyp_params.attn_dropout_t2\n",
        "        self.relu_dropout = hyp_params.relu_dropout\n",
        "        self.res_dropout = hyp_params.res_dropout\n",
        "        self.out_dropout = hyp_params.out_dropout\n",
        "        self.embed_dropout = hyp_params.embed_dropout\n",
        "        self.attn_mask = hyp_params.attn_mask\n",
        "\n",
        "        combined_dim = self.adc_only + self.dwi_only + self.t2_only\n",
        "\n",
        "        self.partial_mode = self.adc_only + self.dwi_only + self.t2_only\n",
        "        if self.partial_mode == 1:\n",
        "            combined_dim = 2 * self.adc_only   # assuming adc_only == dwi_only == t2_only\n",
        "        else:\n",
        "            combined_dim = 2 * (self.adc_only + self.dwi_only + self.t2_only)\n",
        "\n",
        "        output_dim = hyp_params.output_dim        # This is actually not a hyperparameter :-)\n",
        "\n",
        "        # 1. Temporal convolutional layers\n",
        "        self.proj_l = nn.Conv2d(self.orig_d_adc, self.d_l, kernel_size=1, padding=0, bias=False)\n",
        "        self.proj_a = nn.Conv2d(self.orig_d_dwi, self.d_a, kernel_size=1, padding=0, bias=False)\n",
        "        self.proj_v = nn.Conv2d(self.orig_d_t2, self.d_v, kernel_size=1, padding=0, bias=False)\n",
        "\n",
        "        # 2. Crossmodal Attentions\n",
        "        if self.adc_only:\n",
        "            self.trans_l_with_a = self.get_network(self_type='la')\n",
        "            self.trans_l_with_v = self.get_network(self_type='lv')\n",
        "        if self.dwi_only:\n",
        "            self.trans_a_with_l = self.get_network(self_type='al')\n",
        "            self.trans_a_with_v = self.get_network(self_type='av')\n",
        "        if self.t2_only:\n",
        "            self.trans_v_with_l = self.get_network(self_type='vl')\n",
        "            self.trans_v_with_a = self.get_network(self_type='va')\n",
        "\n",
        "        # 3. Self Attentions (Could be replaced by LSTMs, GRUs, etc.)\n",
        "        #    [e.g., self.trans_x_mem = nn.LSTM(self.d_x, self.d_x, 1)\n",
        "        self.trans_l_mem = self.get_network(self_type='l_mem', layers=3)\n",
        "        self.trans_a_mem = self.get_network(self_type='a_mem', layers=3)\n",
        "        self.trans_v_mem = self.get_network(self_type='v_mem', layers=3)\n",
        "\n",
        "        # Projection layers\n",
        "        self.proj1 = nn.Linear(combined_dim, combined_dim)\n",
        "        self.proj2 = nn.Linear(combined_dim, combined_dim)\n",
        "        self.out_layer = nn.Linear(combined_dim, output_dim)\n",
        "\n",
        "    def get_network(self, self_type='l', layers=-1):\n",
        "        if self_type in ['l', 'al', 'vl']:\n",
        "            embed_dim, attn_dropout = self.adc_only, self.attn_dropout\n",
        "        elif self_type in ['a', 'la', 'va']:\n",
        "            embed_dim, attn_dropout = self.dwi_only, self.attn_dropout_dwi\n",
        "        elif self_type in ['v', 'lv', 'av']:\n",
        "            embed_dim, attn_dropout = self.t2_only, self.attn_dropout_t2\n",
        "        elif self_type == 'l_mem':\n",
        "            embed_dim, attn_dropout = 2*self.adc_only, self.attn_dropout\n",
        "        elif self_type == 'a_mem':\n",
        "            embed_dim, attn_dropout = 2*self.dwi_only, self.attn_dropout\n",
        "        elif self_type == 'v_mem':\n",
        "            embed_dim, attn_dropout = 2*self.t2_only, self.attn_dropout\n",
        "        else:\n",
        "            raise ValueError(\"Unknown network type\")\n",
        "\n",
        "        return TransformerEncoder(embed_dim=embed_dim,\n",
        "                                  num_heads=self.num_heads,\n",
        "                                  layers=max(self.layers, layers),\n",
        "                                  attn_dropout=attn_dropout,\n",
        "                                  relu_dropout=self.relu_dropout,\n",
        "                                  res_dropout=self.res_dropout,\n",
        "                                  embed_dropout=self.embed_dropout,\n",
        "                                  attn_mask=self.attn_mask)\n",
        "\n",
        "    def forward(self, x_l, x_a, x_v):\n",
        "\n",
        "        # Project the textual/visual/audio features\n",
        "        proj_x_l = x_l if self.orig_d_adc == self.adc_only else self.proj_l(x_l)\n",
        "        proj_x_a = x_a if self.orig_d_dwi == self.dwi_only else self.proj_a(x_a)\n",
        "        proj_x_v = x_v if self.orig_d_t2 == self.t2_only else self.proj_v(x_v)\n",
        "\n",
        "    # Flatten the features\n",
        "        proj_x_l = proj_x_l.view(proj_x_l.size(0), proj_x_l.size(1), -1).permute(2, 0, 1)\n",
        "        proj_x_a = proj_x_a.view(proj_x_a.size(0), proj_x_a.size(1), -1).permute(2, 0, 1)\n",
        "        proj_x_v = proj_x_v.view(proj_x_v.size(0), proj_x_v.size(1), -1).permute(2, 0, 1)\n",
        "\n",
        "        if self.adc_only:\n",
        "        # (V,A) --> L\n",
        "            h_l_with_as = self.trans_l_with_a(proj_x_l, proj_x_a, proj_x_a)    # Dimension (L, N, adc_only)\n",
        "            h_l_with_vs = self.trans_l_with_v(proj_x_l, proj_x_v, proj_x_v)    # Dimension (L, N, adc_only)\n",
        "            h_ls = torch.cat([h_l_with_as, h_l_with_vs], dim=2)\n",
        "            h_ls = self.trans_l_mem(h_ls)\n",
        "            if type(h_ls) == tuple:\n",
        "                h_ls = h_ls[0]\n",
        "            last_h_l = last_hs = h_ls[-1]   # Take the last output for prediction\n",
        "\n",
        "        if self.dwi_only:\n",
        "        # (L,V) --> A\n",
        "            h_a_with_ls = self.trans_a_with_l(proj_x_a, proj_x_l, proj_x_l)\n",
        "            h_a_with_vs = self.trans_a_with_v(proj_x_a, proj_x_v, proj_x_v)\n",
        "            h_as = torch.cat([h_a_with_ls, h_a_with_vs], dim=2)\n",
        "            h_as = self.trans_a_mem(h_as)\n",
        "            if type(h_as) == tuple:\n",
        "                h_as = h_as[0]\n",
        "            last_h_a = last_hs = h_as[-1]\n",
        "\n",
        "        if self.t2_only:\n",
        "        # (L,A) --> V\n",
        "            h_v_with_ls = self.trans_v_with_l(proj_x_v, proj_x_l, proj_x_l)\n",
        "            h_v_with_as = self.trans_v_with_a(proj_x_v, proj_x_a, proj_x_a)\n",
        "            h_vs = torch.cat([h_v_with_ls, h_v_with_as], dim=2)\n",
        "            h_vs = self.trans_v_mem(h_vs)\n",
        "            if type(h_vs) == tuple:\n",
        "                h_vs = h_vs[0]\n",
        "            last_h_v = last_hs = h_vs[-1]\n",
        "\n",
        "        if self.partial_mode == 3:\n",
        "            last_hs = torch.cat([last_h_l, last_h_a, last_h_v], dim=1)\n",
        "\n",
        "    # A residual block\n",
        "        last_hs_proj = self.proj2(F.dropout(F.relu(self.proj1(last_hs)), p=self.out_dropout, training=self.training))\n",
        "        last_hs_proj += last_hs\n",
        "\n",
        "        output = self.out_layer(last_hs_proj)\n",
        "        return output, last_hs\n",
        "\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "    \"\"\"Multi-headed attention.\n",
        "    See \"Attention Is All You Need\" for more details.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, attn_dropout=0.,\n",
        "                 bias=True, add_bias_kv=False, add_zero_attn=False):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attn_dropout = attn_dropout\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "        self.scaling = self.head_dim ** -0.5\n",
        "\n",
        "        self.in_proj_weight = Parameter(torch.Tensor(3 * embed_dim, embed_dim))\n",
        "        self.register_parameter('in_proj_bias', None)\n",
        "        if bias:\n",
        "            self.in_proj_bias = Parameter(torch.Tensor(3 * embed_dim))\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "\n",
        "        if add_bias_kv:\n",
        "            self.bias_k = Parameter(torch.Tensor(1, 1, embed_dim))\n",
        "            self.bias_v = Parameter(torch.Tensor(1, 1, embed_dim))\n",
        "        else:\n",
        "            self.bias_k = self.bias_v = None\n",
        "\n",
        "        self.add_zero_attn = add_zero_attn\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.in_proj_weight)\n",
        "        nn.init.xavier_uniform_(self.out_proj.weight)\n",
        "        if self.in_proj_bias is not None:\n",
        "            nn.init.constant_(self.in_proj_bias, 0.)\n",
        "            nn.init.constant_(self.out_proj.bias, 0.)\n",
        "        if self.bias_k is not None:\n",
        "            nn.init.xavier_normal_(self.bias_k)\n",
        "        if self.bias_v is not None:\n",
        "            nn.init.xavier_normal_(self.bias_v)\n",
        "\n",
        "    def forward(self, query, key, value, attn_mask=None):\n",
        "        \"\"\"Input shape: Time x Batch x Channel\n",
        "        Self-attention can be implemented by passing in the same arguments for\n",
        "        query, key and value. Timesteps can be masked by supplying a T x T mask in the\n",
        "        `attn_mask` argument. Padding elements can be excluded from\n",
        "        the key by passing a binary ByteTensor (`key_padding_mask`) with shape:\n",
        "        batch x src_len, where padding elements are indicated by 1s.\n",
        "        \"\"\"\n",
        "        qkv_same = query.data_ptr() == key.data_ptr() == value.data_ptr()\n",
        "        kv_same = key.data_ptr() == value.data_ptr()\n",
        "\n",
        "        tgt_len, bsz, embed_dim = query.size()\n",
        "        assert embed_dim == self.embed_dim\n",
        "        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n",
        "        assert key.size() == value.size()\n",
        "\n",
        "        aved_state = None\n",
        "\n",
        "        if qkv_same:\n",
        "            # self-attention\n",
        "            q, k, v = self.in_proj_qkv(query)\n",
        "        elif kv_same:\n",
        "            # encoder-decoder attention\n",
        "            q = self.in_proj_q(query)\n",
        "\n",
        "            if key is None:\n",
        "                assert value is None\n",
        "                k = v = None\n",
        "            else:\n",
        "                k, v = self.in_proj_kv(key)\n",
        "        else:\n",
        "            q = self.in_proj_q(query)\n",
        "            k = self.in_proj_k(key)\n",
        "            v = self.in_proj_v(value)\n",
        "        q = q * self.scaling\n",
        "\n",
        "        if self.bias_k is not None:\n",
        "            assert self.bias_v is not None\n",
        "            k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])\n",
        "            v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])\n",
        "            if attn_mask is not None:\n",
        "                attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1)\n",
        "\n",
        "        q = q.contiguous().view(tgt_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
        "        if k is not None:\n",
        "            k = k.contiguous().view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
        "        if v is not None:\n",
        "            v = v.contiguous().view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
        "\n",
        "        src_len = k.size(1)\n",
        "\n",
        "        if self.add_zero_attn:\n",
        "            src_len += 1\n",
        "            k = torch.cat([k, k.new_zeros((k.size(0), 1) + k.size()[2:])], dim=1)\n",
        "            v = torch.cat([v, v.new_zeros((v.size(0), 1) + v.size()[2:])], dim=1)\n",
        "            if attn_mask is not None:\n",
        "                attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1)\n",
        "\n",
        "        attn_weights = torch.bmm(q, k.transpose(1, 2))\n",
        "        assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]\n",
        "\n",
        "        if attn_mask is not None:\n",
        "            try:\n",
        "                attn_weights += attn_mask.unsqueeze(0)\n",
        "            except:\n",
        "                print(attn_weights.shape)\n",
        "                print(attn_mask.unsqueeze(0).shape)\n",
        "                assert False\n",
        "\n",
        "        attn_weights = F.softmax(attn_weights.float(), dim=-1).type_as(attn_weights)\n",
        "        # attn_weights = F.relu(attn_weights)\n",
        "        # attn_weights = attn_weights / torch.max(attn_weights)\n",
        "        attn_weights = F.dropout(attn_weights, p=self.attn_dropout, training=self.training)\n",
        "\n",
        "        attn = torch.bmm(attn_weights, v)\n",
        "        assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]\n",
        "\n",
        "        attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
        "        attn = self.out_proj(attn)\n",
        "\n",
        "        # average attention weights over heads\n",
        "        attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "        attn_weights = attn_weights.sum(dim=1) / self.num_heads\n",
        "        return attn, attn_weights\n",
        "\n",
        "    def in_proj_qkv(self, query):\n",
        "        return self._in_proj(query).chunk(3, dim=-1)\n",
        "\n",
        "    def in_proj_kv(self, key):\n",
        "        return self._in_proj(key, start=self.embed_dim).chunk(2, dim=-1)\n",
        "\n",
        "    def in_proj_q(self, query, **kwargs):\n",
        "        return self._in_proj(query, end=self.embed_dim, **kwargs)\n",
        "\n",
        "    def in_proj_k(self, key):\n",
        "        return self._in_proj(key, start=self.embed_dim, end=2 * self.embed_dim)\n",
        "\n",
        "    def in_proj_v(self, value):\n",
        "        return self._in_proj(value, start=2 * self.embed_dim)\n",
        "\n",
        "    def _in_proj(self, input, start=0, end=None, **kwargs):\n",
        "        weight = kwargs.get('weight', self.in_proj_weight)\n",
        "        bias = kwargs.get('bias', self.in_proj_bias)\n",
        "        weight = weight[start:end, :]\n",
        "        if bias is not None:\n",
        "            bias = bias[start:end]\n",
        "        return F.linear(input, weight, bias)\n",
        "\n",
        "class SinusoidalPositionalEmbedding(nn.Module):\n",
        "    \"\"\"This module produces sinusoidal positional embeddings of any length.\n",
        "    Padding symbols are ignored, but it is necessary to specify whether padding\n",
        "    is added on the left side (left_pad=True) or right side (left_pad=False).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim, padding_idx=0, left_pad=0, init_size=128):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.padding_idx = padding_idx\n",
        "        self.left_pad = left_pad\n",
        "        self.weights = dict()   # device --> actual weight; due to nn.DataParallel :-(\n",
        "        self.register_buffer('_float_tensor', torch.FloatTensor(1))\n",
        "\n",
        "    @staticmethod\n",
        "    def get_embedding(num_embeddings, embedding_dim, padding_idx=None):\n",
        "        \"\"\"Build sinusoidal embeddings.\n",
        "        This matches the implementation in tensor2tensor, but differs slightly\n",
        "        from the description in Section 3.5 of \"Attention Is All You Need\".\n",
        "        \"\"\"\n",
        "        half_dim = embedding_dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float) * -emb)\n",
        "        emb = torch.arange(num_embeddings, dtype=torch.float).unsqueeze(1) * emb.unsqueeze(0)\n",
        "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1).view(num_embeddings, -1)\n",
        "        if embedding_dim % 2 == 1:\n",
        "            # zero pad\n",
        "            emb = torch.cat([emb, torch.zeros(num_embeddings, 1)], dim=1)\n",
        "        if padding_idx is not None:\n",
        "            emb[padding_idx, :] = 0\n",
        "        return emb\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n",
        "        bsz, seq_len = input.size()\n",
        "        max_pos = self.padding_idx + 1 + seq_len\n",
        "        device = input.get_device()\n",
        "        if device not in self.weights or max_pos > self.weights[device].size(0):\n",
        "            # recompute/expand embeddings if needed\n",
        "            self.weights[device] = SinusoidalPositionalEmbedding.get_embedding(\n",
        "                max_pos,\n",
        "                self.embedding_dim,\n",
        "                self.padding_idx,\n",
        "            )\n",
        "        self.weights[device] = self.weights[device].type_as(self._float_tensor)\n",
        "        positions = make_positions(input, self.padding_idx, self.left_pad)\n",
        "        return self.weights[device].index_select(0, positions.view(-1)).view(bsz, seq_len, -1).detach()\n",
        "\n",
        "    def max_positions(self):\n",
        "        \"\"\"Maximum number of supported positions.\"\"\"\n",
        "        return int(1e5)  # an arbitrary large number\n",
        "\n",
        "def make_positions(tensor, padding_idx, left_pad):\n",
        "    \"\"\"Replace non-padding symbols with their position numbers.\n",
        "    Position numbers begin at padding_idx+1.\n",
        "    Padding symbols are ignored, but it is necessary to specify whether padding\n",
        "    is added on the left side (left_pad=True) or right side (left_pad=False).\n",
        "    \"\"\"\n",
        "    max_pos = padding_idx + 1 + tensor.size(1)\n",
        "    device = tensor.get_device()\n",
        "    buf_name = f'range_buf_{device}'\n",
        "    if not hasattr(make_positions, buf_name):\n",
        "        setattr(make_positions, buf_name, tensor.new())\n",
        "    setattr(make_positions, buf_name, getattr(make_positions, buf_name).type_as(tensor))\n",
        "    if getattr(make_positions, buf_name).numel() < max_pos:\n",
        "        torch.arange(padding_idx + 1, max_pos, out=getattr(make_positions, buf_name))\n",
        "    mask = tensor.ne(padding_idx)\n",
        "    positions = getattr(make_positions, buf_name)[:tensor.size(1)].expand_as(tensor)\n",
        "    if left_pad:\n",
        "        positions = positions - mask.size(1) + mask.long().sum(dim=1).unsqueeze(1)\n",
        "    new_tensor = tensor.clone()\n",
        "    return new_tensor.masked_scatter_(mask, positions[mask]).long()\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer encoder consisting of *args.encoder_layers* layers. Each layer\n",
        "    is a :class:`TransformerEncoderLayer`.\n",
        "    Args:\n",
        "        embed_tokens (torch.nn.Embedding): input embedding\n",
        "        num_heads (int): number of heads\n",
        "        layers (int): number of layers\n",
        "        attn_dropout (float): dropout applied on the attention weights\n",
        "        relu_dropout (float): dropout applied on the first layer of the residual block\n",
        "        res_dropout (float): dropout applied on the residual block\n",
        "        attn_mask (bool): whether to apply mask on the attention weights\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, layers, attn_dropout=0.0, relu_dropout=0.0, res_dropout=0.0,\n",
        "                 embed_dropout=0.0, attn_mask=False):\n",
        "        super().__init__()\n",
        "        self.dropout = embed_dropout      # Embedding dropout\n",
        "        self.attn_dropout = attn_dropout\n",
        "        self.embed_dim = embed_dim\n",
        "        self.embed_scale = math.sqrt(embed_dim)\n",
        "        self.embed_positions = SinusoidalPositionalEmbedding(embed_dim)\n",
        "\n",
        "        self.attn_mask = attn_mask\n",
        "\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for layer in range(layers):\n",
        "            new_layer = TransformerEncoderLayer(embed_dim,\n",
        "                                                num_heads=num_heads,\n",
        "                                                attn_dropout=attn_dropout,\n",
        "                                                relu_dropout=relu_dropout,\n",
        "                                                res_dropout=res_dropout,\n",
        "                                                attn_mask=attn_mask)\n",
        "            self.layers.append(new_layer)\n",
        "\n",
        "        self.register_buffer('version', torch.Tensor([2]))\n",
        "        self.normalize = True\n",
        "        if self.normalize:\n",
        "            self.layer_norm = LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x_in, x_in_k = None, x_in_v = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x_in (FloatTensor): embedded input of shape `(src_len, batch, embed_dim)`\n",
        "            x_in_k (FloatTensor): embedded input of shape `(src_len, batch, embed_dim)`\n",
        "            x_in_v (FloatTensor): embedded input of shape `(src_len, batch, embed_dim)`\n",
        "        Returns:\n",
        "            dict:\n",
        "                - **encoder_out** (Tensor): the last encoder layer's output of\n",
        "                  shape `(src_len, batch, embed_dim)`\n",
        "                - **encoder_padding_mask** (ByteTensor): the positions of\n",
        "                  padding elements of shape `(batch, src_len)`\n",
        "        \"\"\"\n",
        "        # embed tokens and positions\n",
        "        x = self.embed_scale * x_in\n",
        "        if self.embed_positions is not None:\n",
        "            x += self.embed_positions(x_in.transpose(0, 1)[:, :, 0]).transpose(0, 1)   # Add positional embedding\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        if x_in_k is not None and x_in_v is not None:\n",
        "            # embed tokens and positions\n",
        "            x_k = self.embed_scale * x_in_k\n",
        "            x_v = self.embed_scale * x_in_v\n",
        "            if self.embed_positions is not None:\n",
        "                x_k += self.embed_positions(x_in_k.transpose(0, 1)[:, :, 0]).transpose(0, 1)   # Add positional embedding\n",
        "                x_v += self.embed_positions(x_in_v.transpose(0, 1)[:, :, 0]).transpose(0, 1)   # Add positional embedding\n",
        "            x_k = F.dropout(x_k, p=self.dropout, training=self.training)\n",
        "            x_v = F.dropout(x_v, p=self.dropout, training=self.training)\n",
        "\n",
        "        # encoder layers\n",
        "        intermediates = [x]\n",
        "        for layer in self.layers:\n",
        "            if x_in_k is not None and x_in_v is not None:\n",
        "                x = layer(x, x_k, x_v)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "            intermediates.append(x)\n",
        "\n",
        "        if self.normalize:\n",
        "            x = self.layer_norm(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def max_positions(self):\n",
        "        \"\"\"Maximum input length supported by the encoder.\"\"\"\n",
        "        if self.embed_positions is None:\n",
        "            return self.max_source_positions\n",
        "        return min(self.max_source_positions, self.embed_positions.max_positions())\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    \"\"\"Encoder layer block.\n",
        "    In the original paper each operation (multi-head attention or FFN) is\n",
        "    postprocessed with: `dropout -> add residual -> layernorm`. In the\n",
        "    tensor2tensor code they suggest that learning is more robust when\n",
        "    preprocessing each layer with layernorm and postprocessing with:\n",
        "    `dropout -> add residual`. We default to the approach in the paper, but the\n",
        "    tensor2tensor approach can be enabled by setting\n",
        "    *args.encoder_normalize_before* to ``True``.\n",
        "    Args:\n",
        "        embed_dim: Embedding dimension\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, attn_dropout=0.1, relu_dropout=0.1, res_dropout=0.1,\n",
        "                 attn_mask=False):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.self_attn = MultiheadAttention(\n",
        "            embed_dim=self.embed_dim,\n",
        "            num_heads=self.num_heads,\n",
        "            attn_dropout=attn_dropout\n",
        "        )\n",
        "        self.attn_mask = attn_mask\n",
        "\n",
        "        self.relu_dropout = relu_dropout\n",
        "        self.res_dropout = res_dropout\n",
        "        self.normalize_before = True\n",
        "\n",
        "        self.fc1 = Linear(self.embed_dim, 4*self.embed_dim)   # The \"Add & Norm\" part in the paper\n",
        "        self.fc2 = Linear(4*self.embed_dim, self.embed_dim)\n",
        "        self.layer_norms = nn.ModuleList([LayerNorm(self.embed_dim) for _ in range(2)])\n",
        "\n",
        "    def forward(self, x, x_k=None, x_v=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
        "            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n",
        "                `(batch, src_len)` where padding elements are indicated by ``1``.\n",
        "            x_k (Tensor): same as x\n",
        "            x_v (Tensor): same as x\n",
        "        Returns:\n",
        "            encoded output of shape `(batch, src_len, embed_dim)`\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        x = self.maybe_layer_norm(0, x, before=True)\n",
        "        mask = buffered_future_mask(x, x_k) if self.attn_mask else None\n",
        "        if x_k is None and x_v is None:\n",
        "            x, _ = self.self_attn(query=x, key=x, value=x, attn_mask=mask)\n",
        "        else:\n",
        "            x_k = self.maybe_layer_norm(0, x_k, before=True)\n",
        "            x_v = self.maybe_layer_norm(0, x_v, before=True)\n",
        "            x, _ = self.self_attn(query=x, key=x_k, value=x_v, attn_mask=mask)\n",
        "        x = F.dropout(x, p=self.res_dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        x = self.maybe_layer_norm(0, x, after=True)\n",
        "\n",
        "        residual = x\n",
        "        x = self.maybe_layer_norm(1, x, before=True)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, p=self.relu_dropout, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        x = F.dropout(x, p=self.res_dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        x = self.maybe_layer_norm(1, x, after=True)\n",
        "        return x\n",
        "\n",
        "    def maybe_layer_norm(self, i, x, before=False, after=False):\n",
        "        assert before ^ after\n",
        "        if after ^ self.normalize_before:\n",
        "            return self.layer_norms[i](x)\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "def fill_with_neg_inf(t):\n",
        "    \"\"\"FP16-compatible function that fills a tensor with -inf.\"\"\"\n",
        "    return t.float().fill_(float('-inf')).type_as(t)\n",
        "\n",
        "\n",
        "def buffered_future_mask(tensor, tensor2=None):\n",
        "    dim1 = dim2 = tensor.size(0)\n",
        "    if tensor2 is not None:\n",
        "        dim2 = tensor2.size(0)\n",
        "    future_mask = torch.triu(fill_with_neg_inf(torch.ones(dim1, dim2)), 1+abs(dim2-dim1))\n",
        "    if tensor.is_cuda:\n",
        "        future_mask = future_mask.cuda()\n",
        "    return future_mask[:dim1, :dim2]\n",
        "\n",
        "\n",
        "def Linear(in_features, out_features, bias=True):\n",
        "    m = nn.Linear(in_features, out_features, bias)\n",
        "    nn.init.xavier_uniform_(m.weight)\n",
        "    if bias:\n",
        "        nn.init.constant_(m.bias, 0.)\n",
        "    return m\n",
        "\n",
        "\n",
        "def LayerNorm(embedding_dim):\n",
        "    m = nn.LayerNorm(embedding_dim)\n",
        "    return m\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    hyp_params = Namespace(\n",
        "        orig_d_adc=3,\n",
        "        orig_d_dwi=3,\n",
        "        orig_d_t2=3,\n",
        "        adc_only=32,\n",
        "        dwi_only=32,\n",
        "        t2_only=32,\n",
        "        num_heads=16,  # 멀티 헤드 어텐션의 헤드 수\n",
        "        layers=1,  # 어텐션 레이어 수\n",
        "        attn_dropout=0.1,  # 어텐션 드롭아웃 비율\n",
        "        attn_dropout_dwi=0.1,\n",
        "        attn_dropout_t2=0.1,\n",
        "        relu_dropout=0.1,\n",
        "        res_dropout=0.1,\n",
        "        out_dropout=0.1,\n",
        "        embed_dropout=0.1,\n",
        "        attn_mask=True,\n",
        "        output_dim=1  # 출력 차원 (예: 이진 분류의 경우 1)\n",
        "    )\n",
        "\n",
        "    mult_model = MulT(hyp_params)\n",
        "    mult_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7ZBR6uojuYkK"
      },
      "outputs": [],
      "source": [
        "# loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer\n",
        "optimizer = optim.Adam(mult_model.parameters(), lr=params[\"lr\"])\n",
        "# scheduler\n",
        "scheduler = StepLR(optimizer, step_size=40, gamma=params[\"gamma\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = Dataloader(train_data_ADC, train_data_DWI, train_data_T2, batch_size=params[\"batch_size\"])\n",
        "valid_dataloader = Dataloader(valid_data_ADC, valid_data_DWI, valid_data_T2, batch_size=params[\"batch_size\"])\n",
        "test_dataloader = Dataloader(test_data_ADC, test_data_DWI, test_data_T2, batch_size=params[\"batch_size\"])"
      ],
      "metadata": {
        "id": "5zkt2kWj0wsP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fiDO5gaX0RkZ"
      },
      "outputs": [],
      "source": [
        "def extract_features(model, dataloader):\n",
        "    model.eval()\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            adc_images, dwi_images, t2_images, labels = batch\n",
        "            adc_images = adc_images.to(device)\n",
        "            dwi_images = dwi_images.to(device)\n",
        "            t2_images = t2_images.to(device)\n",
        "\n",
        "            outputs = model(adc_images, dwi_images, t2_images)\n",
        "            all_features.append(outputs.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return np.concatenate(all_features, axis=0), np.array(all_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZFdzW5k9wwyO"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for adc_input, dwi_input, t2_input, label in dataloader:\n",
        "        adc_input, dwi_input, t2_input, label = adc_input.to(device), dwi_input.to(device), t2_input.to(device), label.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(adc_input, dwi_input, t2_input)\n",
        "        loss = criterion(outputs, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += label.size(0)\n",
        "        correct += (predicted == label).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate_epoch(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    TP = 0\n",
        "    FN = 0\n",
        "    target_class = 1\n",
        "    with torch.no_grad():\n",
        "        for adc_input, dwi_input, t2_input, label in dataloader:\n",
        "            adc_input, dwi_input, t2_input, label = adc_input.to(device), dwi_input.to(device), t2_input.to(device), label.to(device)\n",
        "\n",
        "            outputs = model(adc_input, dwi_input, t2_input)\n",
        "            loss = criterion(outputs, label)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += label.size(0)\n",
        "            correct += (predicted == label).sum().item()\n",
        "\n",
        "            # Sensitivity for class '1'\n",
        "            TP += (predicted.eq(target_class) & label.eq(target_class)).sum().item()\n",
        "            FN += (predicted.ne(target_class) & label.eq(target_class)).sum().item()\n",
        "\n",
        "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    return epoch_loss, epoch_acc, sensitivity\n",
        "\n",
        "def train(model, train_loader, valid_loader, criterion, optimizer, scheduler, epochs, save_file='model_state_dict.pth', target_class=1):\n",
        "    best_val_accuracy = 0\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state_dict = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
        "        val_loss, val_acc, sensitivity = validate_epoch(model, valid_loader, criterion)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if val_acc > best_val_accuracy or (val_acc == best_val_accuracy and val_loss < best_val_loss):\n",
        "            best_val_accuracy = val_acc\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state_dict = model.state_dict().copy()\n",
        "            print(f\"Epoch {epoch+1}: Saving new best model with val_accuracy {val_acc:.4f}\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss {train_loss:.4f}, Train Accuracy {train_acc:.4f}, Val Loss {val_loss:.4f}, Val Accuracy {val_acc:.4f}\")\n",
        "        print(\"lr: \", optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    if best_model_state_dict:\n",
        "        model.load_state_dict(best_model_state_dict)\n",
        "        torch.save(best_model_state_dict, save_file)\n",
        "\n",
        "    return best_model_state_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTF5RzgA0C38"
      },
      "source": [
        "T-SNE before experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rN9KOB450BTG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "21fef71f-19b3-4b8c-d453-84f73248b429"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 2401.00 GiB. GPU ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-1f033c75627d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 훈련 및 검증 데이터셋의 특성 추출\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmult_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mval_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmult_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 2차원 t-SNE 모델 생성 및 특성 축소\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-380b61d48677>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mt2_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt2_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madc_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdwi_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mall_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mall_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-7b938e24d839>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_l, x_a, x_v)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madc_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# (V,A) --> L\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mh_l_with_as\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrans_l_with_a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproj_x_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj_x_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj_x_a\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Dimension (L, N, adc_only)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0mh_l_with_vs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrans_l_with_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproj_x_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj_x_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj_x_v\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Dimension (L, N, adc_only)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mh_ls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh_l_with_as\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_l_with_vs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-7b938e24d839>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_in, x_in_k, x_in_v)\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx_in_k\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx_in_v\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-7b938e24d839>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, x_k, x_v)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0mx_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_layer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbefore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0mx_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_layer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbefore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres_dropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-7b938e24d839>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, attn_mask)\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0mattn_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbsz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2401.00 GiB. GPU "
          ]
        }
      ],
      "source": [
        "# 훈련 및 검증 데이터셋의 특성 추출\n",
        "train_features, train_labels = extract_features(mult_model, train_dataloader)\n",
        "val_features, val_labels = extract_features(mult_model, valid_dataloader)\n",
        "\n",
        "# 2차원 t-SNE 모델 생성 및 특성 축소\n",
        "tsne_2d = TSNE(n_components=2, random_state=0, perplexity=20)\n",
        "train_features_2d = tsne_2d.fit_transform(train_features)\n",
        "val_features_2d = tsne_2d.fit_transform(val_features)\n",
        "\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "\n",
        "# 2차원 산점도 그리기 - 훈련 데이터셋\n",
        "ax1 = fig.add_subplot(121)\n",
        "ax1.scatter(train_features_2d[train_labels == 0, 0],\n",
        "            train_features_2d[train_labels == 0, 1],\n",
        "            s=50, c='blue', label='g1', alpha=0.5)\n",
        "ax1.scatter(train_features_2d[train_labels == 1, 0],\n",
        "            train_features_2d[train_labels == 1, 1],\n",
        "            s=50, c='red', label='g2', alpha=0.5)\n",
        "ax1.set_title('Training Dataset Before Training')\n",
        "ax1.legend()\n",
        "\n",
        "# 2차원 산점도 그리기 - 검증 데이터셋\n",
        "ax2 = fig.add_subplot(122)\n",
        "ax2.scatter(val_features_2d[val_labels == 0, 0],\n",
        "            val_features_2d[val_labels == 0, 1],\n",
        "            s=50, c='blue', label='g1', alpha=0.5)\n",
        "ax2.scatter(val_features_2d[val_labels == 1, 0],\n",
        "            val_features_2d[val_labels == 1, 1],\n",
        "            s=50, c='red', label='g2', alpha=0.5)\n",
        "ax2.set_title('Validation Dataset Before Training')\n",
        "ax2.legend()\n",
        "plt.savefig(\"before_tsne_plot.png\")\n",
        "mlflow.log_artifact(\"before_tsne_plot.png\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqfn9B5Uzuf1"
      },
      "outputs": [],
      "source": [
        "if mlflow.active_run():\n",
        "    mlflow.end_run()\n",
        "\n",
        "with mlflow.start_run():\n",
        "    mlflow.log_params(params)\n",
        "    best_model_state_dict = train(mult_model, train_dataloader, valid_dataloader, criterion, optimizer, scheduler, params[\"epochs\"])\n",
        "    mult_model.load_state_dict(best_model_state_dict)\n",
        "    for epoch in range(params[\"epochs\"]):\n",
        "        mlflow.log_metric(\"train_loss\", train_losses[epoch], step=epoch)\n",
        "        mlflow.log_metric(\"valid_loss\", val_losses[epoch], step=epoch)\n",
        "        mlflow.log_metric(\"train_acc\", train_accuracies[epoch], step=epoch)\n",
        "        mlflow.log_metric(\"valid_acc\", val_accuracies[epoch], step=epoch)\n",
        "\n",
        "    # 그래프 그리기\n",
        "    epochs_range = range(1, params[\"epochs\"] + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, train_losses, label='Train Loss')\n",
        "    plt.plot(epochs_range, val_losses, label='Valid Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss over Epochs')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, train_accuracies, label='Train Accuracy')\n",
        "    plt.plot(epochs_range, val_accuracies, label='Valid Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy over Epochs')\n",
        "    plt.legend()\n",
        "    plt.savefig(\"plot.png\")\n",
        "    mlflow.log_artifact(\"plot.png\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd6UpTAq0I3s"
      },
      "source": [
        "T-SNE after experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAumm0Y70IAk"
      },
      "outputs": [],
      "source": [
        "# 훈련 및 검증 데이터셋의 특성 추출\n",
        "train_features, train_labels = extract_features(mult_model, train_dataloader)\n",
        "val_features, val_labels = extract_features(mult_model, valid_dataloader)\n",
        "\n",
        "# 2차원 t-SNE 모델 생성 및 특성 축소\n",
        "tsne_2d = TSNE(n_components=2, random_state=0, perplexity=20)\n",
        "train_features_2d = tsne_2d.fit_transform(train_features)\n",
        "val_features_2d = tsne_2d.fit_transform(val_features)\n",
        "\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "\n",
        "# 2차원 산점도 그리기 - 훈련 데이터셋\n",
        "ax1 = fig.add_subplot(121)\n",
        "ax1.scatter(train_features_2d[train_labels == 0, 0],\n",
        "            train_features_2d[train_labels == 0, 1],\n",
        "            s=50, c='blue', label='g1', alpha=0.5)\n",
        "ax1.scatter(train_features_2d[train_labels == 1, 0],\n",
        "            train_features_2d[train_labels == 1, 1],\n",
        "            s=50, c='red', label='g2', alpha=0.5)\n",
        "ax1.set_title('Training Dataset After Training')\n",
        "ax1.legend()\n",
        "\n",
        "# 2차원 산점도 그리기 - 검증 데이터셋\n",
        "ax2 = fig.add_subplot(122)\n",
        "ax2.scatter(val_features_2d[val_labels == 0, 0],\n",
        "            val_features_2d[val_labels == 0, 1],\n",
        "            s=50, c='blue', label='g1', alpha=0.5)\n",
        "ax2.scatter(val_features_2d[val_labels == 1, 0],\n",
        "            val_features_2d[val_labels == 1, 1],\n",
        "            s=50, c='red', label='g2', alpha=0.5)\n",
        "ax2.set_title('Validation Dataset After Training')\n",
        "ax2.legend()\n",
        "plt.savefig(\"after_tsne_plot.png\")\n",
        "mlflow.log_artifact(\"after_tsne_plot.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68_upeof08C1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import mlflow\n",
        "\n",
        "def predict_and_visualize(model, test_loader, model_state_dict_path, output_excel_path='/content/drive/MyDrive/Colab Notebooks/VCMI/20240702/1.xlsx', random_state=params[\"seed\"]):\n",
        "    model.load_state_dict(torch.load(model_state_dict_path))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    preds_list = []\n",
        "    true_list = []\n",
        "    features = []\n",
        "    results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            adc_images, dwi_images, t2_images, labels = batch\n",
        "            adc_images = adc_images.to(device)\n",
        "            dwi_images = dwi_images.to(device)\n",
        "            t2_images = t2_images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(adc_images, dwi_images, t2_images)\n",
        "            probabilities = torch.softmax(outputs, dim=1)\n",
        "            _, preds = torch.max(probabilities, 1)\n",
        "\n",
        "            # 데이터 처리를 각 이미지 세트마다 수행\n",
        "            for i in range(labels.size(0)):\n",
        "                results.append({\n",
        "                    'ADC File Name': os.path.basename(test_loader.dataset.adc_df.iloc[i, 0]),\n",
        "                    'DWI File Name': os.path.basename(test_loader.dataset.dwi_df.iloc[i, 0]),\n",
        "                    'T2 File Name': os.path.basename(test_loader.dataset.t2_df.iloc[i, 0]),\n",
        "                    'True Label': labels[i].item(),\n",
        "                    'Predicted Label': preds[i].item(),\n",
        "                    'Probability g1': probabilities[i][0].item(),\n",
        "                    'Probability g2': probabilities[i][1].item(),\n",
        "                    'Correct': labels[i].item() == preds[i].item()\n",
        "                })\n",
        "\n",
        "            features.append(outputs.cpu().numpy())\n",
        "            preds_list.extend(preds.cpu().numpy())\n",
        "            true_list.extend(labels.cpu().numpy())\n",
        "\n",
        "    features = np.concatenate(features, axis=0)\n",
        "    true_list = np.array(true_list)\n",
        "    preds_list = np.array(preds_list)\n",
        "\n",
        "    print(\"Accuracy for each class:\")\n",
        "    accuracy_g1 = accuracy_score(true_list[true_list == 0], preds_list[true_list == 0])\n",
        "    accuracy_g2 = accuracy_score(true_list[true_list == 1], preds_list[true_list == 1])\n",
        "    print(f\"g1: {accuracy_g1:.4f}\")\n",
        "    print(f\"g2: {accuracy_g2:.4f}\")\n",
        "\n",
        "    balanced_accuracy = balanced_accuracy_score(true_list, preds_list)\n",
        "\n",
        "    tsne = TSNE(n_components=2, random_state=random_state, perplexity=20)\n",
        "    features_2d = tsne.fit_transform(features)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(features_2d[true_list == 0, 0], features_2d[true_list == 0, 1], s=50, c='blue', label='g1', alpha=0.5)\n",
        "    plt.scatter(features_2d[true_list == 1, 0], features_2d[true_list == 1, 1], s=50, c='red', label='g2', alpha=0.5)\n",
        "    plt.title('Test Dataset')\n",
        "    plt.legend()\n",
        "    plt.savefig(\"result_tsne_plot.png\")\n",
        "    mlflow.log_artifact(\"result_tsne_plot.png\")\n",
        "    plt.show()\n",
        "\n",
        "    conf_matrix = confusion_matrix(true_list, preds_list)\n",
        "    TN = conf_matrix[0][0]\n",
        "    TP = conf_matrix[1][1]\n",
        "    FN = conf_matrix[1][0]\n",
        "    FP = conf_matrix[0][1]\n",
        "    sensitivity = TP / (TP + FN)\n",
        "    specificity = TN / (TN + FP)\n",
        "    NPV = TN / (TN + FN)\n",
        "    PPV = TP / (TP + FP)\n",
        "    accuracy = accuracy_score(true_list, preds_list)\n",
        "\n",
        "    metrics_df = pd.DataFrame({\n",
        "        'Metric': ['TP', 'TN', 'FP', 'FN', 'sens', 'spec', 'NPV', 'PPV', 'acc'],\n",
        "        'Value': [TP, TN, FP, FN, sensitivity, specificity, NPV, PPV, accuracy]\n",
        "    })\n",
        "\n",
        "    print(metrics_df)\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_excel(output_excel_path, index=False)\n",
        "\n",
        "\n",
        "    mlflow.log_param(\"random_state\", random_state)\n",
        "    mlflow.log_metric(\"accuracy_g1\", accuracy_g1)\n",
        "    mlflow.log_metric(\"accuracy_g2\", accuracy_g2)\n",
        "    mlflow.log_metric(\"balanced_accuracy\", balanced_accuracy)\n",
        "    mlflow.log_metric(\"sensitivity\", sensitivity)\n",
        "    mlflow.log_metric(\"specificity\", specificity)\n",
        "    mlflow.log_metric(\"NPV\", NPV)\n",
        "    mlflow.log_metric(\"PPV\", PPV)\n",
        "    mlflow.log_metric(\"accuracy\", accuracy)\n",
        "    mlflow.log_artifact(output_excel_path)\n",
        "\n",
        "# 함수 호출 예시\n",
        "predict_and_visualize(mult_model, test_dataloader, 'model_state_dict.pth')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPp8zTnI+Bdt48fSi/gB5yX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}